1. 早上又看了一篇用加法秘密共享的文章，是用在平均共识上的，不太了解这个，可能是数据聚合相关的方向。
   内容其实差不多，我主要是关于浮点数如何处理不太明白，然后发邮件问了一下之前看的文章的作者，人家
   回我了两个方案，一个是直接固定浮点数精度，然后乘个10^m把浮点数补成整数，第二个是用那个尾数阶符
   阶码的浮点数表示法，直接当成32位二进制来处理。

   第二个的方案划分和恢复应该没问题，但这样划分完好像没法进行求和计算；第一个方案可以试下，但得先
   弄清楚这个pytorch的tensor的浮点数的小数最大到多少位，这样统一先乘成整数，分割求和后再除回浮点
   数应该可行。

   然后他们给了我一篇参考文章，题目是Privacy-Preserving Outsourced Calculation on Floating Point Numbers，
   发在tifs上的，打算边看这个边把shamir方案实现了，然后实验部分就完成了，可以开始写文章了。