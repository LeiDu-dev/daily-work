整理了一下和别人撞车的点，感觉可以接着做，但使用知识蒸馏和使用知识转移数据集的动机要换一下，然后突出隐私保护的部分。
总结了一下idea撞车的两篇文章，我们idea雷同的地方是 (1) 允许参与者使用不同的模型进行联邦学习 (2) 通过知识转移数据集实现聚合可以降低通信负担
所以首先去掉第一个点，然后强调第二个点与隐私保护方法的关系

今天想了一下，新的创新点定在：
(1) 首先把环境定在边缘环境下的异步协同学习，依然保留无标签公共数据集的设定，将无标签公共数据集的提供方由服务器改为边缘节点。每个参与者可以从
     边缘节点下载无标签公共数据集，标记后重新上传到边缘节点中，每个参与者只有上传自己的结果后才可以下载其他人的结果。参与者再得到其他人的结果
     之后在本地进行知识蒸馏学习，实现了离线异步协同学习的效果。

     而已有的协同学习方法是同步机器学习，而在边缘环境下每个参与者在不断的移动，难以保持持续稳定的通信，而通过知识蒸馏进行聚合可以实现异步离线
     学习的效果。

(2) 在知识聚合时，如果像已有的方法一样发布模型参数，那么需要对模型进行扰动，但这样做会顾此失彼。为这样虽然实现了共享的模型是安全的，但本地的
     模型在训练时会受到扰动，影响模型精度。所以最好的方法是隔离发布数据与本地模型直接的联系，因此引入无标签公共数据集作为数据聚合的载体，接着
     使用本地化差分隐私技术对发布的结果进行扰动，这样既可以避免本地模型受到影响，也可以保证协同学习的效果。

这样做应该就可以避免雷同了，他们的动机在于差异化模型和低通信负担，而我的动机是异步协同学习+减少差分约束带来的负面影响。

然后现在正在实验，要测试一下这样离线异步学习能不能实现知识共享的效果。

数据发布的隐私保护准备用本地化差分隐私中随机响应的方法，自己试了下影响应该不大，感觉应该行得通。现在就是在看随机扰动的具体实现。
