1. 试了一下在知识蒸馏的时候进行加速就是不再是把知识蒸馏和本地训练彻底分开，而是把知识蒸馏当中本地训练的
   一部分。即原本是知识蒸馏10次，本地训练10次，现在将其变为知识蒸馏3次，本地训练7次，这样训练的总时间就
   不会变慢。然后做实验验证了一下，10+10转换为7+3后，在10轮通信后精度下降了1%，但速度快了几乎一倍。

   在读差分隐私中的函数机制的文章，刚读了个开头，然后就被打断做大组会的汇报PPT了。

   晚上读了一篇4页的短论文发在A会上，内容是差分隐私结合知识蒸馏的，我现在觉得这个内容应该有很多空间可以研究。
   然后这篇文章是对PATE方法的改进，但也就是对softmax输出的概率加噪还有点意思，其他的创新几乎都是别人的东西，
   然后这最有意义的点还因为是短论文所以没有证明过程...

   明天继续读函数机制的文章，争取赶紧把这个集成部分的方案定下来然后验证一下性能。