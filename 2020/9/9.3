1.  把代码改到CIFAR10上运行，因为CIFAR10比较难，所以不同方法的差距能拉开，更容易看出差别。
    然后因为CIFAR10收敛慢，所以写了epoch=100慢慢跑吧，自己计时算了一下，一轮24分钟，100轮
    得跑40多小时...串行太慢了。

    然后再看函数机制，第二遍看又看懂了一些，还差一个项的来源没懂，这个弄懂就可以改差分隐私实现，
    然后应该就可以开始写论文了。

    为了和集成蒸馏区别开，我文章定的出发点是为了在联邦学习时，参与者可以使用不同结构的网络，因为
    网络结构不同就无法使用参数平均，所以使用结果平均的方法。然后引入一个无标签的公共数据集，服务器
    使用接受到的模型集对其进行预测，然后对预测结果取平均值，将预测结果与无表签数据集称为转移数据集，
    用于知识蒸馏。这样相较于共享模型，然后参与者从服务器获取模型集来说，只需要获取打包的数据集即可，
    减少了下行的通信开销，因为转移数据集不需要很大，所以不会又太大的通信开销。

    目前参与者采用同样结构的网络的情况下，我的方法优于传统的模型平均，接近集中训练的性能。
