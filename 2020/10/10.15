主要原因在于pytorch的分布式环境下，多个并行进程调用numpy.random.choice会生成一样的随机数。


由于训练时是10个一组，并行调用基于numpy.random.choice的LDP算法，因此一组中的10个参与者其随机输出是一致的。
因此，对一组数据的输出进行投票，产生的结果与只有一个用户参与的结果没有区别（因为10个结果一样，投票后还是一样）。


也就是说，在该方法下，10个用户对10000个预测结果进行扰动，实际等于只有1个用户，因此表现出45%的准确度。
而参与者上升为100人（即10组）时，只相当于10个用户的效果（因为每10个用户的结果相同，则100个用户等效于10个用户），即88%


结论：
numpy.random.choice在分布式下需要指定随机数种子，而用随机数计算区间落点的方法不需要。
在修改了基于numpy.random.choice的LDP算法的实现后，两种方法在10个参与者时表现出了一样的效果，即准确度由45%提升到88%
即最终效果和下午做报告时最好的结果保持一致，集成学习可以大幅度减弱多元随机对准确度的影响。
