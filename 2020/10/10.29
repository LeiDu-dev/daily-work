1. 在写代码，写了给数据打标记以及用ldp隐私保护的部分，然后写了投票的部分。
   然后这是在epsilon等于1的情况下，扰动数据投票得到伪标签的准确度与投票人数的关系：

   CIFAR10:
   [ 10] 40.39%    [ 10] 83.47%    [ 10] 99.96%
   [ 20] 55.31%    [ 20] 96.7%     [ 20] 100.0%
   [ 30] 66.9%     [ 30] 99.26%    [ 30] 100.0%
   [ 40] 74.8%     [ 40] 99.83%    [ 40] 100.0%
   [ 50] 81.32%    [ 50] 99.95%    [ 50] 100.0%
   [ 60] 86.11%    [ 60] 100.0%    [ 60] 100.0%
   [ 70] 89.55%    [ 70] 100.0%    [ 70] 100.0%
   [ 80] 92.09%    [ 80] 100.0%    [ 80] 100.0%
   [ 90] 93.88%    [ 90] 100.0%    [ 90] 100.0% 
   [100] 95.7%     [100] 100.0%    [100] 100.0%

   MNIST:
   [ 10] 40.19%    [ 10] 84.32%    [ 10] 99.96%
   [ 20] 55.16%    [ 20] 96.98%    [ 20] 99.99%
   [ 30] 65.77%    [ 30] 99.31%    [ 30] 99.99%
   [ 40] 74.3%     [ 40] 99.85%    [ 40] 100.0%
   [ 50] 80.66%    [ 50] 99.94%    [ 50] 100.0%
   [ 60] 85.26%    [ 60] 99.96%    [ 60] 100.0%
   [ 70] 88.95%    [ 70] 99.96%    [ 70] 100.0%
   [ 80] 91.86%    [ 80] 99.98%    [ 80] 100.0%
   [ 90] 94.06%    [ 90] 99.98%    [ 90] 100.0%
   [100] 95.55%    [100] 99.99%    [100] 100.0%

   模型的准确度为92%，扰动后数据保持不变的概率为23%，单个参与者受到隐私保护后的伪标记准确度为21%左右
   可以看到随着投票人数的增加，伪标签准确度逐渐上升，当参与者为100人时，准确度已经超过了无隐私保护下的模型精度。
   说明集成学习的思想可以弥补隐私保护带来的损失

   然后这个准确度应该足够进行实验了，接下来就是跑数据了