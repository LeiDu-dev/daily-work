1. 关于那个切片-洗牌的机制我想了好久，感觉是有问题的。
   然后查了下shuffle在差分隐私的应用，发现这个是最近两年新提出的一种模型机制，一些研究都发表在SODA和FOCS这样的理论计算机顶会上，
   本质上类似于子采样机制，也是一种隐私放大的机制。
   打算花时间研究一下这个，然后在文章里用这个，但这个应该是单纯的理论计算，在代码实现上不需要具体实现。
   所以周末先把代码实现了跑一下，然后理论论证并行吧。


   然后和葛江涛聊了下，决定把动机换成：

   目前的差分隐私联邦学习框架依然是采用基于模型参数的聚合方法，在总隐私预算不变的情况下，随着模型维数的上升，分配给每个数据的隐私预算越来越小，注入的噪声越来越大。
   而基于模型输出的聚合方法可以很好的改善这个问题，因为与大规模的神经网络模型相比，模型的输出维数要小得多。
   
   比如GoogleNet的参数规模为6 998 552个，AlexNet为60 965 244个，VCG16为138 357 544个，而大小为10000的转移数据集，在10分类任务下需要保护的数据量为10万个，
   远远小于模型参数的个数。

   然后联邦学习的框架模型我也给换了，目前应该是独一份，和其他人都不一样的，如果加上shuffle机制，应该算是还不错的创新。
