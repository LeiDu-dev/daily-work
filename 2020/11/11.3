1. 之前的idea在后面输出发布上有问题，进行不下去了，加上那个idea的动机不够好，对比方法也不好对做，所以打算换一个idea。

   基础还是在最早的那个idea，即通过聚合输出来实现联邦学习，但关注放在隐私保护的部分。
   想法是对输出添加噪声，但之前因为输出维数太大了，添加噪声会导致隐私预算急速膨胀，所以做不下去。

   最近看了一个方法应该可以用在这个上面，简单来说就是通过分割和打乱操作，破坏数据间的联系。

   比如有三个参与者ABC，每人分别有三个输出：
   A:  A1 A2 A3
   B:  B1 B2 B3
   C:  C1 C2 C3 

   首先进行分割，将模型输出按样本进行分割，生成每个样本的输出切片(i, Yi)，即
   A:  (1, A1) (2, A2) (3, A3)
   B:  (1, B1) (2, B2) (3, B3)
   C:  (1, C1) (2, C2) (3, C3)

   接着将输出切片打乱顺序，得到
   A:  (2, A2) (3, A3) (1, A1)
   B:  (1, B1) (3, B3) (2, B2)
   C:  (3, C3) (1, C1) (2, C2)

   然后利用伪随机数作为发送间隔，匿名发送输出切片：
   (2, A2) (3, C3) (1, B1) (2, B2) (1, C1) (3, A3) (3, B3) (1, A1) (2, C2)
   
   此时，服务器能接收到完整的输出数据，但由于数据是匿名发送的，且接收间隔无明显联系，故无法将收到的每个数据切片与确定的参与者联系起来。
   这样如果每个输出的隐私预算为e，输出大小为N，迭代次数为T，不进行分割打乱的总隐私预算为e*N*T,而进行分割和打乱则为e，实现了对高维数的处理。


   分割和打乱操作不用具体实现，因为不影响模型的训练。但前面的噪声机制需要实现，然后再根据实验结果调整细节。
   
   idea等于是3个东西的组合(联邦模型蒸馏，高斯噪声机制，分割打乱操作)。
   目前联邦模型蒸馏的隐私保护只有一篇文章，他是通过重采样实现隐私保护，我是在输出中添加噪实现隐私保护。
   分割和打乱操作之前是用在模型权值上的，我这里用在了模型输出上。
   不知道这样算不算创新。

   准备先看一下分割打乱操作的开创文章，是发在SODA上的。我看的使用分割打乱操作的文章是预发表，不过我看有人引用了他的文章，所以结论应该可以保证。
   周四组会细说一下方案，然后赶紧做一下这个的实验。
   