1. 今天看到了一篇知乎上的文章，内容是一篇论文的读书笔记，它里面一个差分隐私深度学习
   的加噪思想是，梯度向量中不是所有量都同等重要，他们认为绝对值大的更重要一些，所有
   把全部梯度都加噪改成了给绝对值大的量加噪。然后我想了下这个思路如果用在分布式中就
   成了给梯度绝对值大的那个节点加更多的噪声，但感觉这个好像不太合适，因为分布式学习
   不同节点产生的贡献好像不能用绝对值大小衡量。
   
   然后又想到一个噪声大小和学习率正相关的思路，即一开始学习率大加大噪声，然后学习率逐
   渐减小，噪声也逐渐减小，我的想法是因为学习率控制着梯度下降的步幅，如果是随机梯度
   下降，那么方向其实有随机性，一开始学习率大的时候对大噪声的容忍性也比较大，然后随着
   学习进行，逐渐趋近收敛，对噪声的容忍性也在变小。但这个思想好像和让噪声随着训练时间
   增加而变小并没有太大的区别...不过让噪声大小和学习率关联起来应该还没有人提出，等代码
   写好也试试这个思路。
   