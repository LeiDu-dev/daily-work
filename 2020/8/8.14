1. 读了今年刚发的文章，Ensemble Distribution Distillation，大体就是现有方法，集成出来的模型只是对结果取平均，但文章认为
   这样做就丢弃了很多有用的消息，应该是对子模型的输出分布进行集成，这样能最大化保留这些信息。

   然后里面提到集成的不确定性=数据不确定性+认知不确定性，其中数据不确定性是来源于训练数据的不可降低的不确定性，而后者是测试
   数据与训练数据不一致而导致的模型认知问题。这个我觉得挺有意思，感觉可以用，但一时半会想不出一个具体思路，加上这个文章是基于
   贝叶斯的，和之前看的还不太一样，所以很多地方看不太懂。打算暂时放一放。