1. 在读文章，然后看了他的方案和安全性证明，比较巧妙的是他在联邦学习中各节点上传参数时用的是样本级的邻近数据集，
   然后在服务器聚合参数后下行广播聚合参数时加噪声用的是节点级邻近数据集的概念，就是某个节点的数据集有区别，而不是
   某个数据集中某个样本有区别，然后就没什么亮点。

   关于实验部分也有点疑问，比如epsilon=60和epsilon=10的损失曲线是差不多的，这个应该不太应该，而且实验结果没有给出模型
   的精度，只给了损失值的曲线。

   还有就是关于隐私预算的计量，如果是传统方法，那隐私预算应该是一个值，然后逐次减去消耗，当消耗完时就结束训练；如果是MA的
   方法，那应该写出用MA来追踪总隐私损失，而且MA是给梯度加噪声的，需要采样，但文章里是给模型加噪声，而且写了各节点训练时的
   batch size=节点样本数，那就等于没有随机采样。这一部分说的不清不楚也很奇怪，明天看下他的收敛性证明的部分，然后就看另一篇
   文章了。