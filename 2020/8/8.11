1. 今天把文章看完了，但有几个疑问。
   第一个是文章中的模型集合用的是多个局部模型的结果平均，比如一个多分类问题，然后用K个局部模型对一个样本做前馈，
   然后把他们的分类结果做个平均。看完文章，应该是这个意思，这个集合的方法和现在常用的相比应该非常简单了，那模型
   聚合那一步就是各个分布式节点把模型参数发给中心服务器，这里应该就是隐私保护的一个点，这里不是其他方法发送梯度，
   而是直接发模型，那只要保证模型的DP就行。但这里如果是发模型，那对于大模型，分布式节点又特别多的情况就不是很实用了，
   不过作者特意把聚合频率调低了，一方面是为了局部模型多样性，另一方面应该也减轻了大模型发送慢的影响吧。

   第二个是文章实验结果里，中心训练的误差居然比一般的分布式大，这个有点奇怪，然后看了下他后来扩充的期刊文章，发现实验把
   中心训练的结果去掉了...不知道到底什么结果

   第三个是压缩模型的部分，这里我的理解是用每个节点用本次的局部模型做初始模型，然后用全局模型打了伪标签的数据去训练，就是
   用局部模型去拟合全局模型，等于是先局部训练t轮，然后聚合后再压缩训练p轮这样，但压缩训练时用的损失函数不是太明白为什么，
   这个后面应该要看一下其他文章的模型压缩实现。

   然后现在打算看一下这篇会议文章扩充的期刊文章，看看有没有什么大的改进。
   然后又找到一篇ICLR2018的文章，是做大规模分布式神经网络通过在线模型蒸馏来训练，那个知乎的文章说这个和集合-压缩的文章出发点不同但思路很相近。打算在看完期刊文章后看下这个，先把非隐私保护的那个框架构思一下，然后看函数机制差分隐私的实现。