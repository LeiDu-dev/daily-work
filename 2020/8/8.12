1. 在读另一篇文章，解决问题的方法很像，但出发点不一样，这个文章的出发点是，分布式中worker到一点数量再增加收益就不大了，
   所以如果worker特别多，比如1000，就把1000分成10份，训练10个分布式模型然后集成再蒸馏缩小集成模型大小。这篇文章的创新应该在蒸馏。

   然后发现集成-压缩是没人做，但人直接把压缩很具体的写成蒸馏，之前的文章虽然说是压缩，但用的是蒸馏这个技术，然后一篇新文章就叫
   集成分布式蒸馏，发到ICLR2020了，突然感觉这应该是个热点。

   现在就是感觉看文章，然后看看能不能抢在前面弄出来。