1. 今天看了large_scale_distributed_neural_network_training_through_online_distillation，然后实验部分没有细看，因为
   这篇文章聚焦的问题是大规模分布式神经网络训练在训练节点数到达一个阈值后如何进一步调动更多的worker来更有效的训练。

   虽然这篇文章的思想和上一篇EC的一样，都是集成+蒸馏，但感觉又不一样。我的看法是上一篇里知识蒸馏主要目的是用来压缩集成模型
   的大小，而这一篇知识蒸馏用在了增强泛化模型性能上。

   然后我想了一个情景，比如边缘计算环境下的一个深度学习任务，每个进入边缘节点范围的个体可以上传自己的模型到边缘节点，同时
   可以也从边缘节点下载其他人上传的模型进行一个知识蒸馏，从而强化本地模型的泛化能力，那这里上传的模型就需要隐私保护。不过
   感觉还是缺少一个点，还需要再想想。