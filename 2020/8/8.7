1. 昨天晚上想差分隐私结合自适应学习率时，想到直接参考已有的自适应学习率算法如何，然后发现RMS，ADMM这些
   优化算法其实就是SGD加上自适应学习率改进来的，然后当时就想那直接把DP加上怎么样，但今天看的这个idea已经被
   人做了，再这个基础上又加上了在梯度向量的不同分量上自适应的加入不同尺度噪声然后发infcom了。

   关于之前想的那个idea一直很不满意，太直白了，谁都能做，没什么价值。

   然后想了很久怎么在分布式学习上搞出特色，想了很久集中学习的一些idea能不能借鉴，但感觉集中学习就像是在一个圆里搞，
   训练的数据就像圆里的点，但分布式学习是好多个圆，就是说分布式学习节点与节点之间的独立性更强。

   （比如集中化学习中在梯度向量的不同分量上加入不同尺度的噪声这样的idea。如果换到分布式学习，在不同节点上加不同大小
   的噪声。但集中化学习梯度向量的分量还是一个整体，虽然不同分量的隐私保护有强有弱，但总体上达标。那如果是分布式学习，
   不能说牺牲某一个节点的安全性，或加强某一个节点的安全性去换总体达标下的性能增强吧。）

   所以就想那不如直接找分布式学习的现有方法，然后去看能不能加入噪声（这里主要是受到上次讲的那个ADMM交替方向算子法的启发，
   但类似的算法好像就这么一个），然后发现大部分分布式学习算法主要还是在解决通信损失的问题，比如有的在解决梯度延迟问题，因为
   通信开销造成的精度损失。

   然后就找到了集成-压缩这个算法，我看这个被引只有2，估计是效果一般，但我看知乎上那个笔记里这个方法的特性还是很适合DP的，所以
   想试一下，如果真的像想象中一样，那如果能在某些情况下好于现在常用的平均算法，我觉得这个创新也好于之前的idea里各种堆砌方法，
   来达成效果要好。

   然后刚才又查了一下这篇文章，发现作者后来又跟进了这个研究，扩充发了一篇期刊论文。现在的想法就是感觉应该有戏，就是不知道这个算不算创新，因为这个集成-压缩的方法是别人的，我只是跟进用在隐私保护上。感觉就像别人发明了SGD，然后我在上面加了个噪声就发了文章这种，好像还得加点什么。