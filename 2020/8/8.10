1. 在看之前说的文章，要做什么已经明白了。

   从理论上讲MA(model averaging)算法是为了解决凸优化提出的，简单来说就是：如果模型关于参数是凸的，损失函数关于输出是凸的，
   那么理论上可以保证全局模型的性能不低于本地模型的平均性能。但是，因为卷积和池化这些存在，深度学习是非凸的，所以在分布式
   深度学习中用MA算法就会造成性能损失。不过大多数用于神经网络的损失函数关于模型输出是凸的，比如交叉熵，因此对本地模型的输出
   进行平均，而不是对其参数进行平均，保证了比本地模型更好的性能。

   但因为模型集成会导致集成后的模型越来越大，所以作者又加了一个模型压缩的步骤，把集成的模型再变小这样，具体用的应该是模型蒸馏，
   就是用集成的模型(teacher model)去给训练数据分类（打标签），然后拿这些带有伪标签的数据去训练一个新的模型(student model)，
   相当于知识的传递。

   现在就是，这个想法很简单，但编程实现肯定要比MA那种求个平均的复杂多了，所以还得花时间看看别人是怎么实现的。

   
   然后最重要的一点就是创新，我觉得如果只是单纯的加DP进去，好像还是有点简单。就像大家都是摊煎饼，但你的创新不是改良了做法，
   只是找了一个新的锅这样，上面的上层建筑都没动。所以我在想，根据之前的研究，当本地模型之间存在显著的多样性时，集成模型往往能取得更好的效果，所以我在想能不能用DP去做这个点，即实现了隐私保护，又引出了多样性，使得集成模型的效果表现的更好。


   想起之前在知乎上看到一个专栏说，目标扰动方法是通过在目标函数中加入随机噪声来构建输出模型的随机性以实现差分隐私，不过目标扰动，函数机制之前没看过，得补课。
