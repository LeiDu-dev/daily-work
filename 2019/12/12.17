1. 看了一篇新文章An Adaptive and Fast Convergent Approach to Differentially Private Deep Learning，
   是王家礼在一个差分隐私qq群里看到的，文章里写已经被INFOCOM2020接收了，然后看了下他的工作感觉应该
   没瞎说。虽然还是给梯度加噪声，但用的不是原始的随机梯度下降，应该是一种新的优化算法的差分隐私改进，
   主要提了两个点，一个是快速收敛，一个是自适应。
   
   1）在快速收敛的实现上使用的是自适应学习率：对模型的不同分量分别调整学习率，加快训练速度，
   缩短收敛时间，从而减少累计隐私损失。
   2）为了减小噪声对模型造成的影响使用了自适应噪声：对模型的不同分量分别计算敏感度，依据敏感度
   的不同添加不同尺度的噪声，从而减少噪声带来的模型精度损失。
   
   然后这篇文章里统计隐私预算用的不是moments accountant，而是使用renyi differential privacy，目前相关研究
   的理解就是(1)找问题(2)定灵敏度(3)确定添加噪声(4)证明安全性(5)算总隐私损失，这样的研究思路。感觉renyi-DP
   应该是蛮重要的概念，我看有两篇文章发表在AISTATS(CCF C)，分别研究了renyi-DP子采样下的隐私放大原理和renyi-DP
   的光滑目标函数下ERM，感觉如果能搞出来renyi-DP在非光滑目标函数下的ERM估计可以发B以上,不过这是机器学习的理论性研究了。
   