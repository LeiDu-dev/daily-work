1. 读了19年TIFS上的文章DP-ADMM: ADMM-Based Distributed Learning With Differential Privacy，
   就是之前说的改进moments accountant到分布式下的那一篇，文中的工作主要基于ADMM交替方向乘子法来做，
   它可以把一个大问题分解为多个小问题从而引入分布式学习。
   
   第一遍看完后有两个地方印象比较深：
   
   一是他用了时变参数，随着迭代次数增加，这个参数大小会发生改变。文章中用这个参数来控制添加的噪声的大小，
   随着迭代次数增进，添加的噪声会逐渐变小使整个算法收敛。但我想这个是不是也可以从另一个角度来看：
   用来控制隐私保护的强度，就是一般情况下隐私保护较弱，然后在几个时间段调整这个参数来放大隐私保护。
   
   其次是文章中的证明过程，基本是照搬的高斯机制的标准证明过程，感觉以后自己来写文章的话，证明部分应该
   可以自己独立完成。
   
   然后就是还有些地方没看懂，差分隐私机制的细节部分，还有就是关于算法收敛的证明，这个感觉对我略难，
   因为没有深入学习过机器学习凸非凸，光滑非光滑的问题。
   
   最后就是有些老文章感觉还是得花时间看一下，比如Calibrating Noise to Sensitivity in Private DataAnalysis
   这篇讲了将噪声校准到函数敏感度，感觉就是差分隐私研究的基石，打算看完手上这篇后看一下。