1. 今天开始学习在机器学习中应用差分隐私的部分，机器学习算法可以被看作是优化问题，
   在满足差分隐私的前提下执行这些优化任务的技术可以分为以下几类：
    1) 直接扰动优化问题的输出
    2) 扰动优化目标函数
    3) 采用现有的优化算法，将需要访问输入数据集的每个独立步骤私密化
    4) 迭代局部搜索
    5) 发布一个针对任务目进行优化的直方图，然后使用该直方图进行优化
   
   今天主要看了前两个类别，
   （1）直接扰动优化问题的输出：
        即通过扰动训练数据集来扰动输出，这种技术需要分析优化问题的敏感度，即当输入数据集D更改一个元组时，
        输出W*会更改多少。然而这些优化问题的敏感度往往很高，以至于这种输出扰动会破坏可用性。在对k-means聚类、
        对数几率回归、线性回归，及支持向量机运用此类技术时，都会因为上述敏感度的原因，破坏算法可用性。						
   （2）扰动优化目标函数：
        书中在这个类型介绍了两个具体的方法：
        1) 一个是直接在代价函数后加了一个噪声，这种方法很难分析向代价函数添加的线性项对其结果准确的的影响，
           但从实验结果上看该方法表现并不好。
        2) 二是用多项式逼近目标函数，然后对多项式的每一个系数进行扰动，从而扰动优化目标函数(和之前大组会上
           第二个讲的思想是一样的)。对于诸如对数几率回归，其目标函数不是有限阶多项式，可以用使用泰勒展开式
           的前两阶项来近似其目标函数。相比于上一种方法，此方法添加的扰动更多，因而性能更差。

   现在主要是先大致看一遍，了解一下基本的方法和思路，具体的细节之后再根据书中和书中给出的参考文献细看。
